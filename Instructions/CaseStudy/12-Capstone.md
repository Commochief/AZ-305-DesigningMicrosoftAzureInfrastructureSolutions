# AZ-305: Designing Microsoft Azure Infrastructure Solutions

Please read the scenario below and develop a proposed Azure architecture to address the scenario as you would for a customer. For the first 20-30 minutes of your presentation, please plan to present your proposed architecture to the. You can use the whiteboard feature or share a file you have developed. Please include a discussion of the pros and cons of your solution as well as any alternatives you considered while developing your proposal. You should attempt to include costing as part of your scenario.

# Scenario 
A Customer component has scalable web application they would like to move to the cloud utilizing Azure services. The Customer web application has the following traffic patterns in production:

- The static number of requests/min during normal business hours (Monday to Friday, 0900 – 1700 ET) is 4,000 requests/minute except as follows: 
- Monday from 0900ET to 1200ET traffic spikes to and is sustained for the duration at 400,000 requests/minute 
- Tuesday through Friday from 0900ET to 1000ET traffic spikes to and is sustained for the duration at 20,000 requests/minute 

The static number of requests/minute outside of normal business hours as specified above is 500 requests/minute.

This application will require a globally distributed Customer Cloud CDN that will route all incoming web requests to the application stack’s load balancer. 80% of the requests originate from North America, 5% of the requests originate from South America, 5% of the requests originate from Europe, 5% of the requests originate from Australia, and 5% of the requests originate from Asia. Static file requests are routed directly to the static file store by the CDN and account for 5% of all requests, averaging 40 KB in size. All web requests are routed to a Customer Cloud load balancer and average 10 KB in size. Load is then distributed evenly across healthy application servers with 10% of all requests resulting in an insert or update to the database. The system utilizes DNS Zone Hosting services offered by the Customer Cloud Broker and averages 1000 DNS requests/hour to a single DNS zone.

The main application resides on multiple nodes that are evenly distributed among the available zones in the region, with a minimum of two nodes at all times. Each application node is hosted on a moderate performance compute instance (8 vCPUs and 32 GB of RAM) and can handle up to 500 requests per minute. During traffic spikes the number of required application instances is expected to grow dynamically using the auto-scaling capabilities available from the Cloud provider. Once a traffic spike has ended, the number of application nodes is expected to scale back down to appropriate baseline levels. All requests are authenticated using transport layer security (TLS).

Application activity will be pushed through a Customer Cloud event stream (also known as a message queue) with functionality like Apache Kafka. Assume 90% of application requests result in events pushed to the queue, an average of 2 events per such requests, and each event is 200 KB in size. Eventually each event is read off the stream and results in an update or write to the database cluster. Users may also upload files to the application, where they will be stored in the static file store and retrieved dynamically according to an application specific access control mechanism. Additionally, the application will store short-term session information in a Customer Cloud caching service. This session storage will start off at 1 GB in size on day 1 (do not price any costs associated with the migration of all of this data to the Customer Cloud), with 100 MB added each day and another 100 MB that expires each day.

The database cluster is hosted currently hosted on six Customer physical compute instances on-premises. The database contains 750 GB of data on day 1 (do not price any costs associated with the migration of all this data to Customer Cloud). The nodes are evenly distributed among the available zones in the region, and each node requires a compute instance with 24 vCPUs, 256 GB of RAM, and 500 GB of normal SSD storage. Every minute, 500 MB of the 750 GB are updated. The data in the relational database will grow in size by 0.2% per month. Each day the reads from the application and analytics nodes total 1.5 TB in size. Automated snapshots are taken of the relational database daily using a Customer Cloud service and rotated every 7 days. Weekly full database backups are created on Saturdays (starting the week the order is placed) and stored online for 4 weeks before being rotated to offline storage.

Four nightly analysis jobs are run on eight high performance RAM optimized compute instances requiring 32 vCPUs and 400GB of RAM each. Each instance should be powered down except for the two hours they run each night. These jobs will each read 700 GB of data from the static file store and 300 GB of data from the database cluster with no data preparation. Each job produces 1 GB of new results which are stored in a separate, redundant, multi-zone Customer Cloud relational database service which requires 8 vCPU, 32 GB of RAM, and 3000 GB of storage. All previous analysis job results are retained. This database service is separate from the database cluster mentioned in the previous paragraphs.

The static file store resides in a single region, will contain 1 TB of static files on day 1, and will grow as necessary (do not price any costs associated with the migration of all this data to Customer Cloud). Every minute, 700 MB of the 1 TB are updated with new content (each update request is 500 KB). The static file store content will grow 0.2% per month. Each day the reads by the application and analytic nodes total 3 TB in size (each read request averages 500 KB).

Of the files uploaded to the static file store, 33% require PKI encryption. Assume this application has one master key, which is managed by the Customer Cloud provider. The application will request separate data keys based on that master for segments of its user population. Assume 5 new keys are created every day. The application will use these keys to encrypt 1000 files per day and decrypt 30,000 per day. 
Every action that changes infrastructure configuration or alters infrastructure state is logged through a Customer Cloud logging service (assume there are 20 such actions a day). Additionally, every data action taken for the managed data services is also logged through the same Customer Cloud service. The infrastructure components running this application are continuously monitored through a Customer Cloud service offering for performance degradation, which sends out an email alert if defined thresholds are exceeded for any infrastructure performance metrics. Additionally, there are 50 separate custom application metrics being monitored.

All virtual machines and app services should be monitored by a common Security Operations Center utilizing a cloud native SIEM/SOAR. The audit logs are required to be retained for one year and should be used for threat analysis and historical modeling of activities.
